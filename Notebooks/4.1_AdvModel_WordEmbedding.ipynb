{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd63b150-f52e-4687-95a6-926984421a12",
   "metadata": {},
   "source": [
    "# Predicting Clinical Trial Terminations\n",
    "### Notebook 4.1: Advanced Modelling - Word Embedding\n",
    "\n",
    "**Author: Clement Chan**\n",
    "\n",
    "---\n",
    "Notes on the notebook:\n",
    "\n",
    "https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT\n",
    "\n",
    "**Bio_ClinicalBERT** model was trained on the [MIMIC III](https://physionet.org/content/mimiciii/1.4/) database that has over 40,000 patients over 11 years\n",
    "\n",
    "- Using the `Study Title`, we will perform word embedding by utilising the Tokenizer from Bio_ClinicalBERT\n",
    "- Then we perform transfer learning by fine-tuning the pretrained model Bio_ClinicalBERT and evaluate metrics like accuracy, f1_score, and roc_auc_score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1373b73-075d-4afd-9717-4da09d42c45e",
   "metadata": {},
   "source": [
    "### Data Dictionary for this notebook that is based on clinicaltrials.gov:\n",
    "\n",
    "---\n",
    "| Column | Description                                  |Data Type|\n",
    "|-------|--------------------------------------------|-------|\n",
    "| Study Status (**Dependant Variable**)| Binary column, 0 for Completed Trials and 1 for Terminated Trials | int |\n",
    "| Study Title | Title of the Clinical Trial           | object |\n",
    "| Brief Summary | Short description of the clinical study (Includes study hypothesis) | object |\n",
    "| Study Results | Whether the results are posted (yes = 1 or no = 0) | int|\n",
    "| Conditions | Primary Disease or Condition being studied     | object |\n",
    "| Primary Outcome Measures | Description of specific primary outcome | object |\n",
    "| Sponsor | The corporation or agency that initiates the study | object |\n",
    "| Collaborators | Other organizations that provide support | object |\n",
    "| Sex | All: No limit on eligibility based on sex, Male: Only male participants, Female: Only female participants | int |\n",
    "| Age | Age group of participants: ADULT, OLDER_ADULT, CHILD  | int |\n",
    "| Phases | Clinical trial phase of the study | int |\n",
    "| Enrollment | Total number of participants in a study | int |\n",
    "| Funder Type | Government, Industry, or Other | int |\n",
    "| Study Type | Interventional = 1, Observational = 0 | int |\n",
    "| Study Design | Study design based on study type | object |\n",
    "| Study Duration | Length of the entire study in categories | object |\n",
    "| Locations | Country of where the clinical study was held | object |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c99af67-d77c-475f-9bd0-b61b4500afe8",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1863211b-466e-40aa-b3ea-2e96cc522ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tingh\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Using the latest cached version of the module from C:\\Users\\tingh\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--accuracy\\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Sat Apr 13 23:27:15 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from C:\\Users\\tingh\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--f1\\0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Sat Apr 13 23:27:20 2024) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from C:\\Users\\tingh\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--roc_auc\\693acedc576861c3f672089d0699a69bbb74f5105e7075204284e52a12f99098 (last modified on Sat Apr 13 23:27:22 2024) since it couldn't be found locally at evaluate-metric--roc_auc, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn metrics\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "# huggingface/evaluate\n",
    "\n",
    "# import PyTorch\n",
    "import torch\n",
    "\n",
    "# huggingface/evaluate (metrics)\n",
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "roc_auc_score = evaluate.load(\"roc_auc\")\n",
    "\n",
    "# huggingface/transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer, TextClassificationPipeline\n",
    "\n",
    "# load_dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ignores the filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcdcf67-5140-4473-9a2d-8b9f603102a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id = 'table'><a/>\n",
    "## Table of Contents\n",
    "\n",
    "---\n",
    "    \n",
    "1. [Evaluating Tokenizer](#token)\n",
    "2. [Text Preprocessing](#text)\n",
    "3. [Transfer Learning](#Transfer)\n",
    "    - a) [TransferModel_V2](#V2)\n",
    "4. [Test Sample](#test)\n",
    "5. [Summary](#sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adef1577-f1c5-4164-be73-5ee8415de18c",
   "metadata": {},
   "source": [
    "[Word Embeddings](https://www.tensorflow.org/text/guide/word_embeddings) gives meaning to related words that have a similar encoding. For example, the words \"cancer\", \"tumor\", and \"maligancy\" are often used as synonyms of each other, so when those sentences and words are vectorized, they should naturally have a smaller cosine similarity. In other words, word embeddings that were trained in a very large database would provide both meaning and better scores when classifying tasks.\n",
    "\n",
    "In this notebook, we will be utilising a pretrained model from HuggingFace called BioClinicalBERT on our Study Title to evaluate how well it classifies terminated trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff66362c-8464-4bb8-a7f3-13ac263229fc",
   "metadata": {},
   "source": [
    "**Initial Setup**\n",
    "- Using both the tokenizer and model from Bio_ClinicalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38735ee4-cc7b-466c-9cf3-679a05483445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bio_ClinicalBert Setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b07e9-ef9a-416c-b165-92ddd19d57d9",
   "metadata": {},
   "source": [
    "**Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f27d3878-78b6-4ed3-9d8b-48ad89ca0989",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Study Title</th>\n",
       "      <th>Study Status</th>\n",
       "      <th>Brief Summary</th>\n",
       "      <th>Study Results</th>\n",
       "      <th>Conditions</th>\n",
       "      <th>Interventions</th>\n",
       "      <th>Primary Outcome Measures</th>\n",
       "      <th>Secondary Outcome Measures</th>\n",
       "      <th>Sponsor</th>\n",
       "      <th>Collaborators</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Phases</th>\n",
       "      <th>Enrollment</th>\n",
       "      <th>Funder Type</th>\n",
       "      <th>Study Type</th>\n",
       "      <th>Study Design</th>\n",
       "      <th>Locations</th>\n",
       "      <th>study_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Effectiveness of a Problem-solving Interventio...</td>\n",
       "      <td>0</td>\n",
       "      <td>We will conduct a two-arm individually randomi...</td>\n",
       "      <td>0</td>\n",
       "      <td>Mental Health Issue (E.G., Depression, Psychos...</td>\n",
       "      <td>behavioral: pride 'step 1' problem-solving int...</td>\n",
       "      <td>Mental health symptoms, The Strengths and Diff...</td>\n",
       "      <td>Mental health symptoms, The adolescent-reporte...</td>\n",
       "      <td>Sangath</td>\n",
       "      <td>Harvard Medical School (HMS and HSDM)|London S...</td>\n",
       "      <td>ALL</td>\n",
       "      <td>CHILD, ADULT</td>\n",
       "      <td>NO PHASE</td>\n",
       "      <td>210-490</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>1</td>\n",
       "      <td>Allocation: RANDOMIZED|Intervention Model: PAR...</td>\n",
       "      <td>Sangath, New Delhi, Delhi, 110016, India</td>\n",
       "      <td>123-244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Investigating the Effect of a Prenatal Family ...</td>\n",
       "      <td>0</td>\n",
       "      <td>The purpose of this study is to measure the di...</td>\n",
       "      <td>0</td>\n",
       "      <td>Focus: Contraceptive Counseling|Focus: Postpar...</td>\n",
       "      <td>other: family planning counseling let by commu...</td>\n",
       "      <td>Self-reported contraceptive use, 6 months post...</td>\n",
       "      <td>Intent to use contraception in the future, 6 m...</td>\n",
       "      <td>Planned Parenthood League of Massachusetts</td>\n",
       "      <td>Society for Family Planning Research Fund</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>CHILD, ADULT, OLDER_ADULT</td>\n",
       "      <td>NO PHASE</td>\n",
       "      <td>120-209</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>1</td>\n",
       "      <td>Allocation: NON_RANDOMIZED|Intervention Model:...</td>\n",
       "      <td>Palestinian Ministry of Health Maternal Child ...</td>\n",
       "      <td>366-515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pre-exposure Prophylaxis (PrEP) for People Who...</td>\n",
       "      <td>1</td>\n",
       "      <td>People who inject drugs (PWID) experience high...</td>\n",
       "      <td>0</td>\n",
       "      <td>Intravenous Drug Abuse</td>\n",
       "      <td>behavioral: prep uptake/adherence intervention...</td>\n",
       "      <td>PrEP uptake by self-report, measured using 1 i...</td>\n",
       "      <td>Participant satisfaction with intervention con...</td>\n",
       "      <td>Boston University</td>\n",
       "      <td>National Institute on Drug Abuse (NIDA)</td>\n",
       "      <td>ALL</td>\n",
       "      <td>ADULT, OLDER_ADULT</td>\n",
       "      <td>NO PHASE</td>\n",
       "      <td>0-8</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>1</td>\n",
       "      <td>Allocation: RANDOMIZED|Intervention Model: PAR...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>245-365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tailored Inhibitory Control Training to Revers...</td>\n",
       "      <td>0</td>\n",
       "      <td>Insufficient inhibitory control is one pathway...</td>\n",
       "      <td>0</td>\n",
       "      <td>Smoking|Alcohol Drinking|Prescription Drug Abu...</td>\n",
       "      <td>behavioral: person-centered inhibitory control...</td>\n",
       "      <td>Inhibitory control performance, Task 1, Perfor...</td>\n",
       "      <td>Far transfer to a task related to inhibitory c...</td>\n",
       "      <td>University of Oregon</td>\n",
       "      <td>none</td>\n",
       "      <td>ALL</td>\n",
       "      <td>ADULT</td>\n",
       "      <td>NO PHASE</td>\n",
       "      <td>80-119</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>1</td>\n",
       "      <td>Allocation: RANDOMIZED|Intervention Model: PAR...</td>\n",
       "      <td>University of Oregon, Social and Affective Neu...</td>\n",
       "      <td>516-671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neuromodulation of Trauma Memories in PTSD &amp; A...</td>\n",
       "      <td>0</td>\n",
       "      <td>The purpose of this study is to examine the ef...</td>\n",
       "      <td>1</td>\n",
       "      <td>Alcohol Dependence|PTSD</td>\n",
       "      <td>drug: propranolol|drug: placebo</td>\n",
       "      <td>Retrieval Session Distress Scores (Session 1),...</td>\n",
       "      <td>Proportion of Drinking Days, Proportion of dri...</td>\n",
       "      <td>Medical University of South Carolina</td>\n",
       "      <td>National Institute on Alcohol Abuse and Alcoho...</td>\n",
       "      <td>ALL</td>\n",
       "      <td>ADULT, OLDER_ADULT</td>\n",
       "      <td>PHASE2</td>\n",
       "      <td>42-59</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>1</td>\n",
       "      <td>Allocation: RANDOMIZED|Intervention Model: PAR...</td>\n",
       "      <td>MUSC, Charleston, South Carolina, 294258908, U...</td>\n",
       "      <td>862-1097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Study Title  Study Status  \\\n",
       "0  Effectiveness of a Problem-solving Interventio...             0   \n",
       "1  Investigating the Effect of a Prenatal Family ...             0   \n",
       "2  Pre-exposure Prophylaxis (PrEP) for People Who...             1   \n",
       "3  Tailored Inhibitory Control Training to Revers...             0   \n",
       "4  Neuromodulation of Trauma Memories in PTSD & A...             0   \n",
       "\n",
       "                                       Brief Summary  Study Results  \\\n",
       "0  We will conduct a two-arm individually randomi...              0   \n",
       "1  The purpose of this study is to measure the di...              0   \n",
       "2  People who inject drugs (PWID) experience high...              0   \n",
       "3  Insufficient inhibitory control is one pathway...              0   \n",
       "4  The purpose of this study is to examine the ef...              1   \n",
       "\n",
       "                                          Conditions  \\\n",
       "0  Mental Health Issue (E.G., Depression, Psychos...   \n",
       "1  Focus: Contraceptive Counseling|Focus: Postpar...   \n",
       "2                             Intravenous Drug Abuse   \n",
       "3  Smoking|Alcohol Drinking|Prescription Drug Abu...   \n",
       "4                            Alcohol Dependence|PTSD   \n",
       "\n",
       "                                       Interventions  \\\n",
       "0  behavioral: pride 'step 1' problem-solving int...   \n",
       "1  other: family planning counseling let by commu...   \n",
       "2  behavioral: prep uptake/adherence intervention...   \n",
       "3  behavioral: person-centered inhibitory control...   \n",
       "4                    drug: propranolol|drug: placebo   \n",
       "\n",
       "                            Primary Outcome Measures  \\\n",
       "0  Mental health symptoms, The Strengths and Diff...   \n",
       "1  Self-reported contraceptive use, 6 months post...   \n",
       "2  PrEP uptake by self-report, measured using 1 i...   \n",
       "3  Inhibitory control performance, Task 1, Perfor...   \n",
       "4  Retrieval Session Distress Scores (Session 1),...   \n",
       "\n",
       "                          Secondary Outcome Measures  \\\n",
       "0  Mental health symptoms, The adolescent-reporte...   \n",
       "1  Intent to use contraception in the future, 6 m...   \n",
       "2  Participant satisfaction with intervention con...   \n",
       "3  Far transfer to a task related to inhibitory c...   \n",
       "4  Proportion of Drinking Days, Proportion of dri...   \n",
       "\n",
       "                                      Sponsor  \\\n",
       "0                                     Sangath   \n",
       "1  Planned Parenthood League of Massachusetts   \n",
       "2                           Boston University   \n",
       "3                        University of Oregon   \n",
       "4        Medical University of South Carolina   \n",
       "\n",
       "                                       Collaborators     Sex  \\\n",
       "0  Harvard Medical School (HMS and HSDM)|London S...     ALL   \n",
       "1          Society for Family Planning Research Fund  FEMALE   \n",
       "2            National Institute on Drug Abuse (NIDA)     ALL   \n",
       "3                                               none     ALL   \n",
       "4  National Institute on Alcohol Abuse and Alcoho...     ALL   \n",
       "\n",
       "                         Age    Phases Enrollment Funder Type  Study Type  \\\n",
       "0               CHILD, ADULT  NO PHASE    210-490       OTHER           1   \n",
       "1  CHILD, ADULT, OLDER_ADULT  NO PHASE    120-209       OTHER           1   \n",
       "2         ADULT, OLDER_ADULT  NO PHASE        0-8       OTHER           1   \n",
       "3                      ADULT  NO PHASE     80-119       OTHER           1   \n",
       "4         ADULT, OLDER_ADULT    PHASE2      42-59       OTHER           1   \n",
       "\n",
       "                                        Study Design  \\\n",
       "0  Allocation: RANDOMIZED|Intervention Model: PAR...   \n",
       "1  Allocation: NON_RANDOMIZED|Intervention Model:...   \n",
       "2  Allocation: RANDOMIZED|Intervention Model: PAR...   \n",
       "3  Allocation: RANDOMIZED|Intervention Model: PAR...   \n",
       "4  Allocation: RANDOMIZED|Intervention Model: PAR...   \n",
       "\n",
       "                                           Locations study_duration  \n",
       "0           Sangath, New Delhi, Delhi, 110016, India        123-244  \n",
       "1  Palestinian Ministry of Health Maternal Child ...        366-515  \n",
       "2                                            unknown        245-365  \n",
       "3  University of Oregon, Social and Affective Neu...        516-671  \n",
       "4  MUSC, Charleston, South Carolina, 294258908, U...       862-1097  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.read_csv('clean_ctg.csv', index_col=0)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25166b6-f5d1-46ba-a932-5a1c641d4141",
   "metadata": {},
   "source": [
    "## 1. Evaluating Tokenizer\n",
    "\n",
    "First let's evaluate the tokenizer to see how it preprocesses the text.\n",
    "- We will select the first 3 Study titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3fa16ae-fb4b-46ed-95ba-e9e262713cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Effectiveness of a Problem-solving Intervention for Common Adolescent Mental Health Problems in India',\n",
       " 'Investigating the Effect of a Prenatal Family Planning Counseling Intervention Led by Community Health Workers on Postpartum Contraceptive Use Among Women in the West Bank',\n",
       " 'Pre-exposure Prophylaxis (PrEP) for People Who Inject Drugs (PWID)']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = text_df['Study Title'][:3].tolist()\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459fdce-08cb-4475-95a3-756842c52cfc",
   "metadata": {},
   "source": [
    "The words inputed into the model must be a fixed-sized tensor. Since the words in the `Study Title` have different lengths, we will need to apply padding and truncation to the tokenizer.\n",
    "\n",
    "\n",
    "**Padding** adds a special [PAD] token to the end of the word sequence to make sure all tensors are the same size.\n",
    "\n",
    "**Truncation** Removes words in the other direction usually corresponding to the `maximum_length`\n",
    "\n",
    "source: (https://huggingface.co/docs/transformers/en/pad_truncation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d746e962-6224-4492-b186-f5427bc700ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 12949,  1104,   170,  2463,   118, 15097,  9108,  1111,  1887,\n",
       "         25506,  4910,  2332,  2645,  1107,  1107,  7168,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101, 11950,  1103,  2629,  1104,   170,  3073, 24226,  1348,  1266,\n",
       "          3693, 22138,  9108,  1521,  1118,  1661,  2332,  3239,  1113,  2112,\n",
       "         17482,  8928, 14255,  4487, 17046,  1329,  1621,  1535,  1107,  1103,\n",
       "          1745,  3085,   102],\n",
       "        [  101,  3073,   118,  7401, 21146, 18873,  7897,  1548,   113,  3073,\n",
       "          1643,   114,  1111,  1234,  1150,  1107, 16811,  5557,   113,   185,\n",
       "         10073,  1181,   114,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer takes in the samples, preprocesses it, and returns \"pytorch\" tensors\n",
    "inputs = tokenizer(sample, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c9965-4520-4cbb-9130-63ab52042fb4",
   "metadata": {},
   "source": [
    "Now we need to convert the input ids back into their corresponding words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71795616-c5a3-492b-92ed-8b7091160004",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'pre',\n",
       " '-',\n",
       " 'exposure',\n",
       " 'prop',\n",
       " '##hyl',\n",
       " '##ax',\n",
       " '##is',\n",
       " '(',\n",
       " 'pre',\n",
       " '##p',\n",
       " ')',\n",
       " 'for',\n",
       " 'people',\n",
       " 'who',\n",
       " 'in',\n",
       " '##ject',\n",
       " 'drugs',\n",
       " '(',\n",
       " 'p',\n",
       " '##wi',\n",
       " '##d',\n",
       " ')',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the third study title and convert the input_IDs back into words\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b56c6f-b2bb-4ae7-89c6-d91891ef151d",
   "metadata": {},
   "source": [
    "Notice how at the beginning of the sequence, the tokenizer adds [CLS] and then [SEP] at the end to signify the start and end of the sequence. The [BERT](https://huggingface.co/docs/transformers/en/model_doc/bert) model stands for Bidirectional Encoder Representations from Transformers. Essentially, BERT encodes the sentence sequence from both directions which is why the sentences needs to be preprocessed a certain way.\n",
    "\n",
    "Additionally, we can see the [PAD] added at the end of the sequence so the tensors are the same size!\n",
    "\n",
    "source (https://huggingface.co/docs/transformers/en/model_doc/bert)\n",
    "\n",
    "Lets look at the model configuration by running `model.config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "39d2d491-70db-4e0e-b32f-7e169aa19cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.32.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb4d46f-f61b-4c15-b795-f2552f1470c1",
   "metadata": {},
   "source": [
    "This looks like our classic neural network where we have our dropout layers to prevent overfitting, the activation methods, learning rate, hidden layers, etc. We can definitely optimize these parameters more in the future!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7d3a56-95a9-45eb-bb24-b523748d6c8c",
   "metadata": {},
   "source": [
    "<a id = 'text'><a/>\n",
    "    \n",
    "## 2. Text Preprocessing\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a82797-db81-4e59-b700-14c0d95802f6",
   "metadata": {},
   "source": [
    "We need to set our labels or the dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb872a9-82e5-4b9d-b6e6-d8808a578eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create expected IDs and their labels\n",
    "id2label = {0: \"COMPLETED\", 1: \"TERMINATED\"}\n",
    "label2id = {\"COMPLETED\": 0, \"TERMINATED\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "988f026b-4554-4239-886f-df22163f1e5b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rnI_pnmZ0AOG",
    "outputId": "8176935c-17b0-4d5c-e4e6-9a6d0cd1f5c9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model setup for text classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a2b72d-493c-413c-b870-04a04ec6f0a0",
   "metadata": {},
   "source": [
    "Then we need to slice out the `Study Status` and the `Study Title` and perform a train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "046604fc-46b6-4ae1-860c-79c2b2de412f",
   "metadata": {
    "id": "vj6u3ULb0AOF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Slice out label and text columns\n",
    "simplified = text_df[[\"Study Status\",\"Study Title\"]].copy()\n",
    "\n",
    "# Rename the columns to \"label\" and \"text\"\n",
    "simplified.columns = [\"label\",\"text\"]\n",
    "\n",
    "# Manual train_test_split\n",
    "test_flag = np.random.randint(0,high=10,size=text_df.shape[0])\n",
    "simplified.loc[:,'test'] = test_flag > 4 # This will split train and test ~50:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5db7142f-2cdc-46b6-a6e7-48be71c17a68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test\n",
       "False    0.50021\n",
       "True     0.49979\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the split proportion\n",
    "simplified['test'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5616ad3c-7816-4809-bf64-e323e398340f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test\n",
       "False    152619\n",
       "True     152491\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can also do value_counts\n",
    "simplified['test'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3b498b-8b4a-4798-be21-d518ae91ea63",
   "metadata": {},
   "source": [
    "Great! Let's do a final check on what the dataframe looks like and then we need to select a sample size to be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "89c348f4-0bbf-4108-9be4-4df40b138d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Effectiveness of a Problem-solving Interventio...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Investigating the Effect of a Prenatal Family ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Pre-exposure Prophylaxis (PrEP) for People Who...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Tailored Inhibitory Control Training to Revers...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Neuromodulation of Trauma Memories in PTSD &amp; A...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text   test\n",
       "0      0  Effectiveness of a Problem-solving Interventio...   True\n",
       "1      0  Investigating the Effect of a Prenatal Family ...  False\n",
       "2      1  Pre-exposure Prophylaxis (PrEP) for People Who...   True\n",
       "3      0  Tailored Inhibitory Control Training to Revers...  False\n",
       "4      0  Neuromodulation of Trauma Memories in PTSD & A...   True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final check\n",
    "simplified.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87beba1b-cc43-4e2b-8404-8ea5a4e37ef5",
   "metadata": {},
   "source": [
    "We will start small by selecting 5000 random samples for the train and 1500 random samples for the test. Then we need to export them as CSVs for them to be processed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f61acf7a-318b-4c11-9d4d-5ff6502018dc",
   "metadata": {
    "id": "a0RctjiA0AOF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select random train sample\n",
    "train = simplified[~simplified.test].groupby('label',group_keys=False).apply(lambda x: x.sample(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fafc2cc-2535-46bf-9786-788c9a513c3a",
   "metadata": {
    "id": "0EU-hwi10AOF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select random test sample\n",
    "test = simplified[simplified.test].groupby('label',group_keys=False).apply(lambda x: x.sample(1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a443157-a7e9-435f-ac0c-0f37d0ea3211",
   "metadata": {
    "id": "b4X3iM3W0AOF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save as CSV files to be processed by the hugging face data pipelines\n",
    "train.reset_index(drop=True).to_csv(\"data/bert_train.csv\")\n",
    "test.reset_index(drop=True).to_csv(\"data/bert_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efeace1e-5934-48c3-a0ff-1f0a7d50d551",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "1ba16397c10c4ed3a8de654fda343e6b",
      "5ec394c1434742fea8f090cdf9961f81",
      "f68a96bc9fcc4bbbbef7ac372518652e",
      "e350bfcdd0b94317b5217b9482313769",
      "98141418b32f423db8558b9c31823f29",
      "92eb39c295a6474fb64cc191a02f7c5f",
      "470613c3869e453b8a24022ff035be69",
      "ec3b223e185a4b98ba753d98a06b0c6e",
      "96695547cbc24421b545c3e4f2f1c258",
      "9c052a34f70d45618ce0ecc9c4b3c3a1",
      "6af587058ade47b5b9c4212dacea7999",
      "5b2141a2b74f404192c0d01390301bee",
      "3348dbe0a3e84b958816154e9d7d6f30",
      "b7e02e2ce6404060a9da268bdd0d01da",
      "853ef9c3db2840c5a432df35035160e5",
      "7b58ab3c665945da957f0b09b61cb5f0",
      "e981466cd17c4deba5029cb5e4d97e15",
      "f2f612811f614fe0b54f8283721a7f5f",
      "3c3dd95c7370454087dc153a39ba6bc0",
      "37b2c9df1a5e4c88b2de72b5576affa6",
      "4ccf484ce9294231b95648ee5cd1dede",
      "3669cc7e258e4fa4951dadb6db8c8be3",
      "f253498053574cbe89fee5316a19cd70",
      "b950d2ad96a44a69ab4da133d2a0a735",
      "48d2ddf0547e4649a556029d1c0400f8",
      "086f236ff1f34369b2332a71cae3b337",
      "9d9656ae8f6948c7aa150cb6b1de5f97",
      "578e829bceab412abede56f2c0f1f8b6",
      "9cd81c6c918945b39ff79c0799ebf18b",
      "3f30fbde685a4c2fa3e6d74065b8b6e0",
      "0d6e01efa37f4459ba52766613c369e4",
      "f2d272fb3eaa43eca0ace48147cbf66b",
      "abc6a88127904b71ba38aa065c5fccb4",
      "d91e476234e347da90dcede5f59be589",
      "72a0a0bc4c7b4d6b82980fe639260ca4",
      "71f9d84578ab41bf87f3e7c6609313d9",
      "caa717f942f14b529768e9909d0f1b87",
      "f50b514915094450a5665b2494f4c450",
      "c036a6c056f64bb0ae8b08a3bf770536",
      "4ea13320846c4f1a89ce5461d8d9795d",
      "c2877e5890d444aeb0dbc441662ae76e",
      "f792004b38054bb5b8c6c5b3ba1a7579",
      "81c9b60563504494a1844c6dba303fd3",
      "d862eaa392be44c794ab0b8d105f8305"
     ]
    },
    "id": "k4nRFU9M0AOF",
    "outputId": "efc902f1-9ae3-4fed-ceaa-a0b80f96e0b4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/tingh/.cache/huggingface/datasets/csv/default-6aa0f91d2967186f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5d2b0146dc4dd3983d0a0403758490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19b3bb59a024627ab6ef344ee6194cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/tingh/.cache/huggingface/datasets/csv/default-6aa0f91d2967186f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470703e8507f456ca017c4816ba1ac91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load as hugging face dataset\n",
    "dataset = load_dataset('csv', data_files={'train': 'data/bert_train.csv', 'test': 'data/bert_test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c87a985c-2549-4672-ba31-cc8b138a74e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (10000, 4), 'test': (3000, 4)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7fae4-e336-422d-969c-bf24ba2bb2f4",
   "metadata": {},
   "source": [
    "Great this looks good, now let's move on to fine-tuning the pretrained model!\n",
    "\n",
    "<a id = 'transfer'><a/>\n",
    "    \n",
    "## 3. Transfer Learning - Fine-Tuning Pretrained Model (BioClinicalBERT)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92a9a88-5078-4775-a9d3-51d7c2713508",
   "metadata": {},
   "source": [
    "We need to define a preprocess function that uses the tokenizer and parameters we mentioned before.\n",
    "- dataset.map will map the preprocess function to be applied to all rows of the `Study Title`\n",
    "- **DataCollatorWithPadding** helps prepare batches by dynamically padding the sequences with a common length\n",
    "\n",
    "source (https://huggingface.co/docs/transformers/en/main_classes/data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a5379fb-3d04-4956-ad92-27de75716c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "# From hugging face text processing setup\n",
    "tokenized_data = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd5ece-4b1d-4af2-89ec-2564dd8c10e1",
   "metadata": {},
   "source": [
    "We also need to define our metrics!\n",
    "- HuggingFace has their own evaluate library that consists of many evaluation metrics for their models.\n",
    "- We will be using accuracy, f1_score, and auc_score to evaluate our model's performance.\n",
    "\n",
    "Source (https://huggingface.co/docs/evaluate/en/index)\n",
    "\n",
    "**Important Notes**\n",
    "\n",
    "We need to run an np.argmax() function since the predictions will provide two scores for each class. The argmax() will find the largest score out of the 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4594a60-faa7-4dff-af72-402043ce55b8",
   "metadata": {
    "id": "EvWYMko40AOG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# leverage sklearn metrics for runtime training evaluation\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy_score = accuracy.compute(predictions=pred, references=labels)\n",
    "    f1_score = f1.compute(predictions=pred, references=labels)\n",
    "    auc_score = roc_auc_score.compute(prediction_scores=pred, references=labels)\n",
    "\n",
    "    return {\"accuracy\": accuracy_score, \"f1\": f1_score, \"AUC\": auc_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b82d588-0b33-4c75-9895-c97331198d2d",
   "metadata": {
    "id": "-4agrqa00AOG"
   },
   "source": [
    "Next we can setup our complete transfer learning pipeline by creating a TrainingArguments instance with specific parameters and creating a new Trainer that collects all components together: model, training_args, preprocessing pipeline, and evaluation funcs.\n",
    "\n",
    "**Training_args**\n",
    "- output_dir: saves the checkpoints of the model\n",
    "- weight_decay: is the models regularlization that prevents overfitting (higher number = stronger regularlization)\n",
    "- evaluation_strategy: computes the metrics after every epoch\n",
    "- save_strategy: saves a checkpoint after every epoch\n",
    "- load best model at the end!\n",
    "\n",
    "source (https://huggingface.co/docs/transformers/en/main_classes/trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4128ee1f-db42-4f26-b688-517b9f840dbb",
   "metadata": {
    "id": "6LDXVsFJ0AOG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training parameter setup goes in a specific class instance\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"clinical-study-classifier\",\n",
    "    learning_rate=2e-5,\n",
    "    # optim=\"adamw_torch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False)\n",
    "\n",
    "# the trainier\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e230d71-7964-42d9-a95c-a365db2c0e37",
   "metadata": {},
   "source": [
    "Finally! We can now use trainer.train() to start fine-tuning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7662707-0076-4f26-8392-b76b8d172ffb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12500' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/12500 1:01:51, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.669700</td>\n",
       "      <td>0.654717</td>\n",
       "      <td>{'accuracy': 0.6116666666666667}</td>\n",
       "      <td>{'f1': 0.5881937080240367}</td>\n",
       "      <td>{'roc_auc': 0.6116666666666667}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>0.677115</td>\n",
       "      <td>{'accuracy': 0.608}</td>\n",
       "      <td>{'f1': 0.6072144288577155}</td>\n",
       "      <td>{'roc_auc': 0.6079999999999999}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.479200</td>\n",
       "      <td>0.833124</td>\n",
       "      <td>{'accuracy': 0.6033333333333334}</td>\n",
       "      <td>{'f1': 0.62882096069869}</td>\n",
       "      <td>{'roc_auc': 0.6033333333333334}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>1.412217</td>\n",
       "      <td>{'accuracy': 0.5953333333333334}</td>\n",
       "      <td>{'f1': 0.5879158180583842}</td>\n",
       "      <td>{'roc_auc': 0.5953333333333333}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>1.956136</td>\n",
       "      <td>{'accuracy': 0.5983333333333334}</td>\n",
       "      <td>{'f1': 0.6192733017377566}</td>\n",
       "      <td>{'roc_auc': 0.5983333333333333}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>2.654079</td>\n",
       "      <td>{'accuracy': 0.591}</td>\n",
       "      <td>{'f1': 0.6027840725153771}</td>\n",
       "      <td>{'roc_auc': 0.591}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.101100</td>\n",
       "      <td>3.079926</td>\n",
       "      <td>{'accuracy': 0.5873333333333334}</td>\n",
       "      <td>{'f1': 0.5780504430811179}</td>\n",
       "      <td>{'roc_auc': 0.5873333333333333}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.063600</td>\n",
       "      <td>3.385440</td>\n",
       "      <td>{'accuracy': 0.5786666666666667}</td>\n",
       "      <td>{'f1': 0.5552427867698804}</td>\n",
       "      <td>{'roc_auc': 0.5786666666666667}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.044400</td>\n",
       "      <td>3.506462</td>\n",
       "      <td>{'accuracy': 0.5886666666666667}</td>\n",
       "      <td>{'f1': 0.5951443569553806}</td>\n",
       "      <td>{'roc_auc': 0.5886666666666667}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>3.573226</td>\n",
       "      <td>{'accuracy': 0.5903333333333334}</td>\n",
       "      <td>{'f1': 0.5843760568143388}</td>\n",
       "      <td>{'roc_auc': 0.5903333333333334}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12500, training_loss=0.27236922760009763, metrics={'train_runtime': 3712.528, 'train_samples_per_second': 26.936, 'train_steps_per_second': 3.367, 'total_flos': 5052624373834080.0, 'train_loss': 0.27236922760009763, 'epoch': 10.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a4535-609a-448a-97bb-5db06bf7e2d7",
   "metadata": {},
   "source": [
    "The model is overfitting to the training data because the training loss is going down while the validation loss is going up! Our highest Accuracy is around ~61% which is not that good, the f1_score ranges from 55% to 63% and the AUC scores ranges from 57% - 61%.\n",
    "\n",
    "This is by any means not a good model since the scores are not optimal. This is probably due to the small dataset that is fed into the model. To prevent overfitting, we can increase the `weight_decay` in the next attempt and add more rows into the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f2843097-99c4-4620-85e2-4d34dfd696fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# saving the model\n",
    "trainer.save_model(\"models/TransferModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff427fdf-8787-4704-9eba-73733d29f2fd",
   "metadata": {},
   "source": [
    "<a id = 'V2'><a/>\n",
    "\n",
    "### 3. a) TransferModel_V2\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eeb081-427e-4201-96d1-a502b87c33e6",
   "metadata": {},
   "source": [
    "Using the same code, we will slice out 7500 random samples for train and 2500 random samples for test. Unfortunately due to limited time and processing power, we cannot add a larger sample size to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aa191f3-4777-4107-bf88-ef8bbe8ce0a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select random train sample\n",
    "train = simplified[~simplified.test].groupby('label',group_keys=False).apply(lambda x: x.sample(7500))\n",
    "\n",
    "# Select random test sample\n",
    "test = simplified[simplified.test].groupby('label',group_keys=False).apply(lambda x: x.sample(2500))\n",
    "\n",
    "# save as CSV files to be processed by the hugging face data pipelines\n",
    "train.reset_index(drop=True).to_csv(\"data/bert_train.csv\")\n",
    "test.reset_index(drop=True).to_csv(\"data/bert_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75cfd77e-b45c-43b3-85a6-4e6f5666706c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/tingh/.cache/huggingface/datasets/csv/default-f2b70ceaad06f926/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efdeb8b08594596b03f447664e21e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5daea95ad82466fb080809fac3bdae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/tingh/.cache/huggingface/datasets/csv/default-f2b70ceaad06f926/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa503f9e45234b6281f17b08253b6bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train': (15000, 4), 'test': (5000, 4)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load as hugging face dataset\n",
    "dataset = load_dataset('csv', data_files={'train': 'data/bert_train.csv', 'test': 'data/bert_test.csv'})\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e7fa2-1ed7-4619-a952-d55f37ffc711",
   "metadata": {},
   "source": [
    "We need to remap the processing function again to the new dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d7157b5-4728-4233-94fa-10bdb3c05976",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# From hugging face text processing setup\n",
    "tokenized_data = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bbfb40-4531-4d9f-9e32-f9d0aa73ea62",
   "metadata": {},
   "source": [
    "Keeping all parameters the same and increasing weight_decay to 0.1 to prevent overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b02cd43-6685-481e-9f2b-4cd894444af8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training parameter setup goes in a specific class instance\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"clinical-study-classifier\",\n",
    "    learning_rate=2e-5,\n",
    "    # optim=\"adamw_torch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.1, # changed from 0.01 to prevent overfit\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False)\n",
    "\n",
    "# the trainier\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82a55ea2-5693-46e6-a1c4-8ae0866eccb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18750/18750 1:23:51, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.669100</td>\n",
       "      <td>0.653622</td>\n",
       "      <td>{'accuracy': 0.6236}</td>\n",
       "      <td>{'f1': 0.6818796484110886}</td>\n",
       "      <td>{'roc_auc': 0.6235999999999999}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.620800</td>\n",
       "      <td>0.666562</td>\n",
       "      <td>{'accuracy': 0.6312}</td>\n",
       "      <td>{'f1': 0.6285253827558421}</td>\n",
       "      <td>{'roc_auc': 0.6312000000000001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.497900</td>\n",
       "      <td>0.767860</td>\n",
       "      <td>{'accuracy': 0.6244}</td>\n",
       "      <td>{'f1': 0.6370313103981446}</td>\n",
       "      <td>{'roc_auc': 0.6244000000000001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.348300</td>\n",
       "      <td>1.301880</td>\n",
       "      <td>{'accuracy': 0.6158}</td>\n",
       "      <td>{'f1': 0.5903177649818725}</td>\n",
       "      <td>{'roc_auc': 0.6157999999999999}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>1.776627</td>\n",
       "      <td>{'accuracy': 0.6162}</td>\n",
       "      <td>{'f1': 0.6488563586459287}</td>\n",
       "      <td>{'roc_auc': 0.6162000000000001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>2.269892</td>\n",
       "      <td>{'accuracy': 0.6056}</td>\n",
       "      <td>{'f1': 0.6038569706709521}</td>\n",
       "      <td>{'roc_auc': 0.6055999999999999}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.127300</td>\n",
       "      <td>2.623639</td>\n",
       "      <td>{'accuracy': 0.6112}</td>\n",
       "      <td>{'f1': 0.637853949329359}</td>\n",
       "      <td>{'roc_auc': 0.6112000000000001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>2.835454</td>\n",
       "      <td>{'accuracy': 0.6052}</td>\n",
       "      <td>{'f1': 0.6228505922812381}</td>\n",
       "      <td>{'roc_auc': 0.6052000000000001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.055100</td>\n",
       "      <td>3.152989</td>\n",
       "      <td>{'accuracy': 0.6002}</td>\n",
       "      <td>{'f1': 0.6276774073384244}</td>\n",
       "      <td>{'roc_auc': 0.6002}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>3.290262</td>\n",
       "      <td>{'accuracy': 0.606}</td>\n",
       "      <td>{'f1': 0.6247619047619047}</td>\n",
       "      <td>{'roc_auc': 0.6060000000000001}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18750, training_loss=0.29077237213134766, metrics={'train_runtime': 5032.6505, 'train_samples_per_second': 29.805, 'train_steps_per_second': 3.726, 'total_flos': 7099685940078240.0, 'train_loss': 0.29077237213134766, 'epoch': 10.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1e1ed-8a9d-4c31-8849-d7ddc980f60f",
   "metadata": {},
   "source": [
    "The model is still overfitting to the training set, but is performing much better than the previous tuned model. This model has a accuracy of 62%, F1_score of 68% and AUC_score of 62% which is not optimal but an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2c9f2f9-b738-407d-9bb5-18576620eba5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# saving the model\n",
    "trainer.save_model(\"models/TransferModel_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff24b97-a8fb-4cad-bb99-bdce61fe25cb",
   "metadata": {},
   "source": [
    "<a id = 'test'><a/>\n",
    "\n",
    "## 4. Test Sample\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca8d99f-4dd0-4558-8bb5-827fe2c6f437",
   "metadata": {},
   "source": [
    "We can perform some sample tests to see what the model would predict given a `Study Title`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "116603c1-22b7-4cd7-b6a4-7629457999e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([     2,     15,     17,     48,     49,     60,     64,     65,     79,\n",
       "           94,\n",
       "       ...\n",
       "       305044, 305045, 305052, 305057, 305065, 305075, 305078, 305081, 305085,\n",
       "       305092],\n",
       "      dtype='int64', length=41730)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list out terminated trial indices\n",
    "text_df[text_df['Study Status'] == 1]['Study Title'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11abf7b5-4858-4a87-b818-80e9c0c1b34b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sunitinib and Capecitabine for First Line Colon Cancer'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick a random index to plug into pipeline\n",
    "term_sample = text_df[text_df['Study Status'] == 1]['Study Title'][48]\n",
    "term_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78819b7b-9980-4813-872d-29c4f000037b",
   "metadata": {},
   "source": [
    "The TextClassificationPipeline is a simple pipe that classifies text based on the fine-tuned model and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aab087d0-7fc4-4051-a134-b94ff026f8e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'TERMINATED', 'score': 0.780062735080719}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions for the entire dataset\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=False, device='cuda') # cuda allows the function to run on your GPU\n",
    "pipe([term_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66178db2-66ce-4865-9531-03ebcdb8ee4a",
   "metadata": {},
   "source": [
    "Great! It's predicting the `Study Title` as Terminated with a confidence of 78%\n",
    "\n",
    "Let's try it with just the word `COVID`, since in the previous notebook, we found that COVID related trials had a higher coefficient to terminated trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cd03eb76-ad8a-4499-852f-19376cf789cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'TERMINATED', 'score': 0.5402730703353882}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions for the entire dataset\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=False, device='cuda') # cuda allows the function to run on your GPU\n",
    "pipe([\"COVID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c28f64-74c4-48f8-83dc-211b608e30ff",
   "metadata": {},
   "source": [
    "This is looking good! Now we just need to test out some of the completed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "472b7cf2-fc5b-4945-a2dc-87fec7934450",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([     0,      1,      3,      4,      5,      6,      7,      8,      9,\n",
       "           10,\n",
       "       ...\n",
       "       305100, 305101, 305102, 305103, 305104, 305105, 305106, 305107, 305108,\n",
       "       305109],\n",
       "      dtype='int64', length=263380)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list out completed trial indices\n",
    "text_df[text_df['Study Status'] == 0]['Study Title'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b7520bd-148e-4fdd-88bf-36a1f9769b70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Pilot Study to Assess the Effectiveness of BehaviouRal ActiVation Group Program in Patients With dEpression: BRAVE'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick a random index to plug into pipeline\n",
    "comp_sample = text_df[text_df['Study Status'] == 0]['Study Title'][7]\n",
    "comp_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5cc62ea9-6577-4ece-9f2a-0f835fb137d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'COMPLETED', 'score': 0.597963809967041}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions for the entire dataset\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=False, device='cuda')\n",
    "pipe([comp_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b337c87-c3c5-42b8-859b-6c885902fe71",
   "metadata": {},
   "source": [
    "Now it managed to predict a completed trial based on the `Study Title` with a confidence of 60%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd63cf-64a6-4047-970b-fbe0ee3d0bb6",
   "metadata": {},
   "source": [
    "<a id = 'sum'><a/>\n",
    "\n",
    "## 5. Summary\n",
    "\n",
    "---\n",
    "    \n",
    "We created word embeddings from the `Study Title` column and then performed Transfer Learning by feeding those embedded words into a pretrained model. Our final model in this notebook manages to predict trial terminations at a 62% accuracy with a f1_score of 68% and a AUC_score of 62%. To improve this model further we can add more rows into the model and tokenize the `Brief Summary` column as well. Additionally, it would be useful to learn how to optimize the BERT configuration and training arguments further to obtain better results\n",
    "    \n",
    "**Limitations:**\n",
    "Text preprocessing and fine-tuning pretrained model requires a lot of time, iterations, and computing power. It will be useful to learn how to optimize training more efficiently to get better results in a shorter time.\n",
    "    \n",
    "**Product Demo**\n",
    "We will include this model in our product demo, since it will be useful to find out what words in `Clinical Study Titles` often associate with a trial terminations\n",
    "\n",
    "---\n",
    "    \n",
    "### Predicting Clinical Trial Termination Conclusion\n",
    "    \n",
    "Our initial goal was to find out the main factors associated with trial terminations and then create a predictive machine learning model to classify terminated trials.\n",
    "\n",
    "**Some factors include**\n",
    "- Low enrollments (0-8)\n",
    "- Trials associated with COVID\n",
    "- Chronic conditions such as cancer, parkinsons, crohn disease etc.\n",
    "- Specific Sponsors and Collaborators\n",
    "- And many more...\n",
    "    \n",
    "We created 2 predictive models, a fine-tuned RandomForest Classifier and a word embedded pretrained model.\n",
    "    \n",
    "**Next Steps:** We can definitely refine the each of the models more based on the limitations, incorporate further hyperparameter tuning, and different feature selection strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Clinical_Trial_env",
   "language": "python",
   "name": "clinical_trial_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
